## Sparkify Data Lake

The purpose of this project is to perform ETL to insert song and log data from a music streaming application into a Data Lake in S3. This is done using Apache Spark running on an EMR cluster. The resulting data can then be used to analyse user behaviour . 

### Requirements
* Apache Spark 2.4.4 +
* Amazon Web Services (AWS)
* Amazon Elastic MapRecue (EMR)
* Amazon S3
* Python 3.6 + 
* Pyspark 2.4.3 +
* Jupyter/Ipython

### Description of Data Sets:

#### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

#### Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json

The log data set has the following columns:
'artist',  'auth',  'firstName',  'gender',  'itemInSession',  'lastName',  'length',  'level',  'location',  'method',  'page',  'registration',  'sessionId',  'song',  'status',  'ts',  'userAgent',  'userId'

### Description of Data Model:
We import data into Spark from the log and song files which are stored in Amazon S3. This data, after processing in Spark, is written out to a bunch of partitioned Parquet files (one set of Parquet files for each of the tables) in S3. Let's now take a look at the tables.

#### Dimension tables
1. songs: contains information about songs. This is fetched from the song data. Columns: song_id, title, artist_id, year, duration.
2. artists: contains information about the songs' artists. This is fetched from the song data.
3. time: contains information about when songs were played. This information is present in the log files. The timestamp is broken down into specific units. Columns: start_time, hour, day, week, month, year, weekday.
4. users: a table containing user information. The user information is obtained from the log files. Columns: user_id, first_name, last_name, gender, level.

#### Fact table
1. songplays: contains information pertaining to the event that a song is played. Contains user, time, artist and song info. Columns: songplay_id, start_time, user_id, level, song_id, artist_id, location, session_id, user_agent.

### Description of files in repository
The log and song data files are present in S3. These are Json (Jsonline) files which are stored (respectively) at:
* Song data: s3://udacity-dend/song_data
* Log data: s3://udacity-dend/log_data. 
An overview of the different files in the repository is given below. 

#### Python programs
1. etl.py: The ETL program which extracts data from the log files as well as the song files, processes them using Spark on an EMR cluster and and loads them into S3 in Parquet format.

#### Ipython notebooks
1. etl.ipynb: A notebook which was used for a trial run using sample data. This trial run writes the output parquet files into the local machine.

#### data and parquet folders
The data/ folder contains sample song and log data, while the parquet/ folder contains sample output (output from etl.ipynb).

### Execution
1. Create a key pair on EC2. Save the pem file into Linux (or PPK file for Windows).
2. Create a new EMR cluster with Spark and m5.xlarge nodes. Make sure to select the created key pair. 
3. Add an inbound rule for SSH in the master node's security group: see https://docs.amazonaws.cn/en_us/emr/latest/ManagementGuide/emr-gs-ssh.html.
4. ssh into the Spark cluster using the key in the PEM file: ssh -i ~/spark-cluster.pem hadoop@ec2...
5. Copy etl.py and the pem file (cfg file not necessary) into the cluster.
1. Run spark-submit -- master yarn etl.py to submit the job.
